from fastapi import FastAPI
from pydantic import BaseModel
from transformers import AutoTokenizer, AutoModelForCausalLM
from fastapi.middleware.cors import CORSMiddleware
import torch
import uvicorn

# åˆå§‹åŒ– FastAPI
app = FastAPI(title="Medical Chatbot API", description="Qwen åŒ»ç–—é—®ç­”æ¥å£", version="1.0")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # æˆ–è€…æ”¹æˆ ["http://localhost:3000"] ç­‰å‰ç«¯åœ°å€
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# å®šä¹‰è¯·æ±‚ä½“
class Query(BaseModel):
    question: str

# åŠ è½½æ¨¡å‹ï¼ˆåªåŠ è½½ä¸€æ¬¡ï¼‰
MODEL_NAME = "Qwen/Qwen2.5-0.5B-Instruct"
print("ğŸš€ æ­£åœ¨åŠ è½½æ¨¡å‹ï¼Œè¯·ç¨ç­‰â€¦â€¦")
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    dtype=torch.float16,
    device_map="auto",
    trust_remote_code=True
)
print("âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼")

@app.post("/ask")
def ask_medical_bot(query: Query):
    messages = [
        {"role": "system", "content": "ä½ æ˜¯ä¸€åä¸“ä¸šåŒ»ç”Ÿï¼Œè¯·ç”¨ç®€æ˜ã€å‡†ç¡®çš„æ–¹å¼å›ç­”åŒ»å­¦é—®é¢˜ã€‚"},
        {"role": "user", "content": query.question}
    ]

    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    inputs = tokenizer(text, return_tensors="pt").to(model.device)

    outputs = model.generate(
        **inputs,
        max_new_tokens=200,
        temperature=0.6,
        top_p=0.9,
        repetition_penalty=1.2
    )

    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return {"answer": answer}

@app.get("/")
async def root():
    return {"message": "åŒ»å­¦ Chatbot API æ­£åœ¨è¿è¡Œ ğŸš€"}

# å¯åŠ¨æœåŠ¡
if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8002)

