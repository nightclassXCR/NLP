from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
from evaluate import load

# æ¨¡å‹è·¯å¾„ï¼ˆä½¿ç”¨å¾®è°ƒåçš„ï¼‰
MODEL_PATH = "./finetuned_qwen_medical"
tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(MODEL_PATH, device_map="auto", torch_dtype=torch.float16)

# åŠ è½½è¯„æµ‹é›†
data = load_dataset("json", data_files="data/eval_data.json")["train"]

# æŒ‡æ ‡
bleu = load("bleu")
rouge = load("rouge")
bertscore = load("bertscore")

preds, refs = [], []

# æ¨ç†ç”Ÿæˆ
for i, sample in enumerate(data):
    inputs = f"æŒ‡ä»¤ï¼š{sample['instruction']}\nå›ç­”ï¼š"
    inputs_tokenized = tokenizer(inputs, return_tensors="pt").to(model.device)

    with torch.no_grad():
        outputs = model.generate(
            **inputs_tokenized,
            max_new_tokens=128,
            temperature=0.7,
            do_sample=True,
        )

    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)
    # å»æ‰promptå‰ç¼€
    answer = answer.split("å›ç­”ï¼š")[-1].strip()

    preds.append(answer)
    refs.append(sample["output"])

# è‡ªåŠ¨è¯„æµ‹
bleu_score = bleu.compute(predictions=preds, references=[[r] for r in refs])
rouge_score = rouge.compute(predictions=preds, references=refs)
bert_score = bertscore.compute(predictions=preds, references=refs, lang="zh")

print("\nğŸ“Š æ¨¡å‹è¯„æµ‹ç»“æœï¼š")
print(f"BLEU: {bleu_score['bleu']:.4f}")
print(f"ROUGE-L: {rouge_score['rougeL']:.4f}")
print(f"BERTScore-F1: {sum(bert_score['f1']) / len(bert_score['f1']):.4f}")
print("âœ… è¯„æµ‹å®Œæˆï¼")
