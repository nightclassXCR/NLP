from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    Trainer,
    TrainingArguments,
    DataCollatorForLanguageModeling
)
import torch

MODEL_NAME = "Qwen/Qwen2.5-0.5B-Instruct"

print("ğŸš€ åŠ è½½æ¨¡å‹ä¸­â€¦â€¦")
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token  # è¡¥å…… pad_token

model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    device_map="cuda:0",
    trust_remote_code=True
)
print("æ¨¡å‹è®¾å¤‡ï¼š", model.device)
print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")

# åŠ è½½å¹¶æ ¡éªŒæ•°æ®é›†
dataset = load_dataset("json", data_files="./data/filtered_data.json")
required_columns = ["instruction", "output"]
for col in required_columns:
    if col not in dataset["train"].column_names:
        raise ValueError(f"æ•°æ®é›†ç¼ºå°‘å¿…è¦å­—æ®µï¼š{col}")
# è¿‡æ»¤ç©ºæ–‡æœ¬æ ·æœ¬
dataset = dataset.filter(lambda x: len(x["instruction"].strip()) > 0 and len(x["output"].strip()) > 0)
print(f"è¿‡æ»¤åæ•°æ®é›†å¤§å°ï¼š{len(dataset['train'])} æ¡")

# é¢„å¤„ç†å‡½æ•°ï¼ˆé€‚é… batched=Trueï¼‰
def preprocess_function(examples):
    # ä½¿ç”¨ Qwen å®˜æ–¹å¯¹è¯æ¨¡æ¿ç›´æ¥ç”Ÿæˆ token id åˆ—è¡¨ï¼ˆå·²å®Œæˆç¼–ç ï¼‰
    chats_token_ids = []
    for ins, out in zip(examples["instruction"], examples["output"]):
        # ç”Ÿæˆå•æ ·æœ¬çš„ token id åˆ—è¡¨
        token_ids = tokenizer.apply_chat_template(
            [{"role": "user", "content": ins}, {"role": "assistant", "content": out}],
            add_generation_prompt=False,
            truncation=True,  # æˆªæ–­è¿‡é•¿æ–‡æœ¬
            max_length=512,   # é™åˆ¶æœ€å¤§é•¿åº¦
        )
        # æ‰‹åŠ¨ padding åˆ° max_lengthï¼ˆè‹¥ä¸è¶³åˆ™ç”¨ pad_token_id å¡«å……ï¼‰
        if len(token_ids) < 512:
            token_ids += [tokenizer.pad_token_id] * (512 - len(token_ids))
        # ç¡®ä¿ä¸è¶…è¿‡ max_lengthï¼ˆé˜²æ­¢æˆªæ–­ä¸ç”Ÿæ•ˆï¼‰
        token_ids = token_ids[:512]
        chats_token_ids.append(token_ids)

    # ç›´æ¥æ„å»º model_inputsï¼ˆæ— éœ€äºŒæ¬¡ç¼–ç ï¼‰
    model_inputs = {
        "input_ids": chats_token_ids,
        "attention_mask": [[1 if id != tokenizer.pad_token_id else 0 for id in seq] for seq in chats_token_ids]
    }

    # æ‰¹é‡å¤„ç† labels
    input_ids_batch = torch.tensor(model_inputs["input_ids"], dtype=torch.long)
    batch_size, seq_len = input_ids_batch.shape
    labels_batch = torch.full((batch_size, seq_len), -100, dtype=torch.long)

    # æŸ¥æ‰¾ <|assistant|> çš„ token id å¹¶æ©ç ç”¨æˆ·è¾“å…¥éƒ¨åˆ†
    assistant_token_id = tokenizer.convert_tokens_to_ids("<|assistant|>")
    if assistant_token_id == -1:
        raise ValueError("Tokenizer æœªè¯†åˆ« <|assistant|> tokenï¼Œè¯·æ›´æ–° transformers åº“å’Œæ¨¡å‹")

    assistant_mask = torch.tensor([[id == assistant_token_id for id in seq] for seq in input_ids_batch], dtype=torch.bool)

    for i in range(batch_size):
        pos = assistant_mask[i].nonzero(as_tuple=True)[0]
        if len(pos) > 0:
            start_idx = pos[0] + 1
            if start_idx < seq_len:
                labels_batch[i, start_idx:] = input_ids_batch[i, start_idx:]

    model_inputs["labels"] = labels_batch.tolist()
    return model_inputs

print("ğŸ”§ é¢„å¤„ç†ä¸­â€¦â€¦")
tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=dataset["train"].column_names)
print("âœ… é¢„å¤„ç†å®Œæˆï¼Œå…±", len(tokenized_dataset["train"]), "æ¡æ•°æ®")

# æ•°æ® Collator
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False
)

# è®­ç»ƒå‚æ•°
training_args = TrainingArguments(
    output_dir="./results",
    per_device_train_batch_size=1,
    gradient_accumulation_steps=4,
    num_train_epochs=0.1,
    logging_steps=5,
    logging_first_step=True,
    save_strategy="no",
    optim="adamw_torch",
    fp16=False,
    report_to="none",
    gradient_checkpointing=True,
    max_grad_norm=1.0,
       learning_rate=5e-6,  # è°ƒä½å­¦ä¹ ç‡
)

# å¯åŠ¨è®­ç»ƒ
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"],
    data_collator=data_collator,
)

trainer.train()

# ä¿å­˜æ¨¡å‹
model.save_pretrained("./medical_qwen_finetuned")
tokenizer.save_pretrained("./medical_qwen_finetuned")
print("âœ… å¾®è°ƒå®Œæˆï¼æ¨¡å‹å·²ä¿å­˜åˆ° ./medical_qwen_finetuned")